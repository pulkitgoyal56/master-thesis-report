% The Discussion should begin with a statement of the important findings of the study. Subsequent sections can address technical issues, analysis of the results, and the implications of the work. Again, it is often helpful to break down the Discussion into subsections that focus on particular topics. It is proper to include a section that summarizes and expands upon conclusions that may be drawn from the work (raises open questions, proposes future studies).
\chapter{Discussion}
\label{sec:discussion}

% Summary of CLIP reward analysis
Semantic expression is an innate nature of human exploration during creative free-play.
Motivated by these observations in cognitive science and developmental psychology, in this thesis, we try to leverage multimodal foundational models to realize this behavior in artificial agents, i.e. bias exploration in free play towards semantically meaningful and visually expressive states.

Towards this end, we formulate the semantics entropy reward using the semantic inductive bias from the large vision language model CLIP as a means to give the agent a freedom akin to free play in humans and enable it to reach semantically meaningful states.

To test the efficacy of this reward in archiving rich and diverse creative expression, we extended the ShapeGridWorld environment and created the Tangram environment.
We demonstrate the performance of CLIP in these abstract settings across parameters and establish that CLIP is noisy and consequently has problems such as sparse rewards and semantic biases in random images.

Thus we develop methods to improve its use for our purposes, where we build upon recent work on baseline regularization in using VLMs as a source of goal-conditioned rewards to formulate the regularized semantics entropy reward.
Furthermore, we adapt and devise other additional methods such as negative embeddings, post-suffixes, and tweaking the rendering function (adding texturing or modifying the images with other operations) to improve the reward signal.

Subsequently, in our analyses, we show how these techniques affect the reward trajectories, agnostic to the variations due to prompt engineering.
The different additional methods have subtly disparate desired effects on the reward signal, but we find the baseline regularization strengths and temperature of the softmax activation function to be the most effective in reducing CLIP's noise.
Their effects are complementary, but not quite the same.

Baseline regularization improves the shape of the reward landscape by considering tradeoffs between directions of effective semantic change in the CLIP embedding space.
The delta embeddings capture the important aspects of the desired change in the text embeddings so the reward function can focus on imbuing this change in the image observation space, and the absolute embeddings help in providing context.
By considering a tradeoff between the two as a hyperparameter in either modality, we try to balance these two important aspects.
We find that moderate values of the two baseline regularization hyperparameters (\(\sim (0.3, 0.3)\)) are the most effective in guiding the agent to semantically meaningful states.
Overall, we think that baseline regularization makes the reward landscape denser and softens/improves its gradients.

However, a consequence of this is poorer adversarial performance, i.e. a larger semantic bias in random and imperfect images, making the final creations less expressive.
This is tackled by the temperature parameter, which complements baseline regularization, and at values between \(0.01 - 0.02\), smoothens the reward landscape and reduces this semantic bias.
The resulting reward trajectories have less noise and the creations are less abstract.
A similar effect is achieved by the shearing operation on the images.

We use these insights from our results to guide the hyperparameter choice and search in the main experiments using the regularized semantics reward.
We find that tweaking just the regularization strengths and temperature is generally enough to achieve good reward signals.
Moreover, these hyperparameters are robust and slight variations in their values in the given ranges do not affect the reward signal significantly.
The modified reward is effective in consistently guiding the agent to semantically meaningful states and enables it to create a range of semantically expressive creations in both environments.

Although, these creations can still show a lack of expressiveness.
This degenerative effect can be attributed to poorly shaped (flat) reward signals near goal states of absolute meaning and could be exacerbated by artifacts of the environment design or limitations of the agent's capabilities since the controller uses finite-horizon planning which hinders it from finding global optima.

Regardless, this is further mitigated by the complementary regularity reward (RaIR) which improves the quality of these creations.
This is especially evident in Tangram where the resulting creations are more recognizable and appealing.
In ShapeGridWorld, the creations with RaIR are neater and more organized, with fewer random pixels.
RaIR particularly archives this in the later stages of the rollout, by incentivizing the agent to modify its creation while maintaining its semantic value.
Effectively, it helps in escaping and navigating different local minima, encouraging exploration, and in turn, leads to more meaningful creations with degrees of abstraction.

\subsection*{Limitations and Future Work}
\label{sec:future-work}

The final creations the agent achieves still suffer from a lack of diversity as we are not able to overcome the class preference of CLIP.
Some categories such as ``house'' in Tangram are chosen overwhelmingly more often than others.
In ShapeGridWorld, where we mostly used letters and numbers as creative possibilities, the agent prefers the letter ``N'' followed by the letters ``R'' and ``S'' over others.
Even removing these categories just gave rise to other categories that were preferred by CLIP.
(Counts of all the creations we observed in our simulations are summarized in \figref{fig:class-preference-tangram} and \figref{fig:class-preference-sgw} in the appendix.)

These can be the inherent biases in the CLIP model, perhaps emerging from structures in natural language.
They could also be due to the specific shapes of these creations and how they might be more suited or be easier to stumble upon in the environments we use.
For example, in Tangram, a house can simply be created with a face-up triangle above a square.
The agent also prefers to make a hammer with a parallelogram and a square, and a heart with simply a red triangle pointing down.

% Bottlenecked by open-sourced vision-language models â†’ if we cannot denoise the CLIP signal well enough, we can just use perhaps ChatGPT4 with vision to do some experiments

We think these behaviors are related to the capability limitations in the current open-source multimodal foundational models.
Perhaps using newer vision-language models from OpenCLIP such as \texttt{ViT-bigG-14} \citep{openclip} which are trained on the bigger LAION-5B dataset \citep{laion5b} as reward models might lead to some performance improvements, with better generalization, reduced noise, and diminished biases.

% Flatnet
We also explore the possibility of de-noising CLIP by fine-tuning with entropy regularization and re-training with random images with our toy models vis-\`a-vis \emph{flatnet}.
We got some promising results with the toy models, so we think scaling it to the full CLIP model could improve results further.

% Comments on the environments
Future work can also experiment with giving the agent control over the choice of object to move.
This is a harder control problem and was not done in our experiments for the same reason.
We wanted to reduce the complexity of the problem and focus on the reward signal.
Similarly, experiments could be conducted in complex or realistic environments, with more objects, elaborate shapes, or additional dimensions.

% Future work
Other improvements could be using temporally adaptive weights for the regularity reward to explicitly force the mechanism that we observed with RaIR in improving expression, i.e. starting with a smaller \(\lambda\) to encourage semantic expression and then increasing it later.
These phases could also be shorter and interleaved to ensure that the agent does not degenerate to local minima.

Furthermore, multiple terms for regularity can be introduced, for example, a local regularity in every object's limited vicinity with higher precision and a global more coarse regularity with higher granularity (see \eqref{eq:rair-relational-extended}).
This is motivated by observations that structures in the real world are regular at different scales in a similar manner, and might lead to more complex/richer creations.

Our experiments were limited to fully observable MDPs with known dynamics using ground-truth models.
Similarly, the application of RaIR requires object-centric state-space representations where the observations are disentangled per object.
In our case, this is manually specified in the form of the precise positions of all the objects.
Such representations that can capture relational inductive biases of the objects-object interactions in the environment could instead be learned.
Testing the semantics reward in partially observable environments with learned models (with or without object-centric representations) could be a good direction for future work.

\subsection*{Conclusion}
\label{sec:conclusion}
In this work, we show that the semantics entropy reward can be used to bias exploration in free play towards semantically meaningful states, and with the help of baseline regularization methods, lead to rich and diverse semantic expression.
Additionally, regularity and compression help promote this expression by grounding and coalescing the creations in better forms and patterns.

% Comments on creativity
The resulting creations are reminiscent of real objects and show a degree of abstraction and creativity.
% Comments on exploration
When used as an additional reward to augment traditional novelty-seeking intrinsic rewards \(r_{\text{intrinsic}} = r_{\text{semantics}} + r_{\text{novelty}}\), the semantics entropy reward can lead to more human-like exploration i.e. curious behaviors with semantically-sound creative expression.
