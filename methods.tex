% The Methods section should be brief but adequate to allow a qualified investigator to repeat the research. The animals, supplies and equipment used should be described in detail. All companies from which compute resources were obtained should be listed with their location. All methods of analysis and statistical testing must be identified and explained in detail. Studies employing animals have to identify that the experiments being reported were approved by the Institutional Animal Care and Use Committee.
\chapter{Methods}
\label{sec:methods}

We use a family of VLMs called CLIP \citep{clip} for semantic inference and a model-based planning controller using an improved Cross-Entropy Method (iCEM) \citep{icem} as our artificial agent.
This section describes the components of our approach, including the mathematical setup, the CLIP model, the intrinsic semantics rewards, and the iCEM controller.

\section{Preliminaries}
\label{sec:preliminaries}
% - Notation
We consider environments that can be described by a fully observable Markov Decision Process (MDP), given by the tuple \((\cS, \cA, \cO, \theta, f, r)\), with state-space \(\cS \in \nR^{n_s}\), action-space \(\cA \in \nR^{n_a}\), observation-space (image-space) \(\cO \in \nR^{n_o}\), rendering function \(\theta: \cS \mapsto \cO\), transition kernel (environment model) \(f: \cS \times \cA \mapsto \cS\), and reward function \(r: \cS \times \cO \mapsto \nR\).

A trajectory \(\bmtau_t = \{(\bfs_t, \bfo_t, R_t, \bfa_t), (\bfs_{t+1}, \bfo_{t+1}, R_{t+1}, \bfa_{t+1}), \ldots \}\) at time \(t\) is a sequence of observation-action pairs starting from the current state \(\bfs_t\), where \(\bfs_k \in \cS\), \(\bfs_k \in \cO\), \(R_k \in \nR\), and \(\bfa_k \in \cA\).

In a typical RL setting, the returns of such a trajectory \(\bmtau_t\) are the discounted sum of rewards given by,
\begin{equation}
    \label{eq:rl-goal}
    g(\bmtau_t) = \sum_{k=0} \gamma^k R_{t+k},
\end{equation}
where \(R_t = r(\bfs_t, \bfo_t)\) is the expected reward at time \(t\) and \(\gamma \in [0, 1]\) is the discount factor. The agentâ€™s goal is to learn an action policy that maximizes its expected reward;
\begin{equation}
    \label{eq:rl-policy}
    \pi^* = \argmax_{\pi} E_{\bmtau \sim \pi} \left[ g(\bmtau) \right],
\end{equation}
where the expectation is under trajectories \textit{rolled-out} with \(\bfa_t \sim \pi(\cdot \mid \bfs_t)\), \(\bfs_{t+1} = f(\bfs_t, \bfa_t)\), \(\bfo_t = \theta(\bfs_t)\), and \(R_t = r(\bfs_t, \bfo_t)\).

In our formulations, we consider a finite-horizon problem, where the agent is tasked with maximizing the expected return over a limited planning horizon \(h\).

\section{Vision-Language Models}
\label{sec:vlms}

We define vision-language models (VLMs; \cite{vlmsurvey}) as functions capable of processing both language inputs \(\bml \in \cL\) and vision inputs \(\bmi \in \cI\), where \(\cL\) is a set of natural language strings/prompts and \(\cI \subseteq \cO\) is the space of 2D RGB images.

\subsection{CLIP}
\label{sec:clip}
CLIP (Contrastive Language Image Pretraining; \cite{clip}) is a family of vision-language models that have two main components -- an image encoder \(\psi_I: \cI \mapsto \cV\) and a language encoder \(\psi_L: \cL \mapsto \cV\), which generate unit magnitude vector embeddings for images and text, respectively, in the same high-dimensional latent space \(\cV \in \nR^{n_v}\), which can thus be compared to each other using a vector similarity metric \(\zeta: \cI \times \cL \mapsto \nR\), e.g. cosine similarity (\(S_c\), essentially dot product),
\begin{equation}
    \label{eq:clip-similarity}
    \zeta(\bfi, \bfl) = S_c(\psi_I(\bfi), \psi_L(\bfl)) = \frac{\psi_I(\bfi) \cdot \psi_L(\bfl)}{\| \psi_I(\bfi) \| \| \psi_L(\bfl) \|} = \psi_I(\bfi) \cdot \psi_L(\bfl).
\end{equation}
Eponymously, CLIP encoders are jointly trained using a contrastive loss which directs the model to semantically align the labeled image and text pairs (in the latent space) by increasing their similarity while simultaneously pushing apart the embeddings of the mismatched pairs by decreasing their similarity.

\subsubsection{CLIP as Zero-Shot Image Classifier}
\label{sec:clip-classifier}
CLIP can be used as a zero-shot classifier by comparing the embeddings of a given image and a set of prompts to predict the likelihood of the image belonging to all of the classes.

Given an image \(\bfi\) and a set of text prompts \(\bml\), the likelihood of the image belonging to the class described by a prompt \(\bfl\) is given by the softmax over the similarity scores of the image and the prompt embeddings,
\begin{equation}
    \label{eq:clip-dist}
    p(\bfl; \bfi, \bml, T) = \frac{\exp(\zeta(\bfi, \bfl)/T)}{\sum_{\bfl' \in \bml} \exp(\zeta(\bfi, \bfl')/T)},
\end{equation}
where \(T\) is the temperature hyperparameter that controls the flatness of the distribution.

\section{Semantic Rewards for Free Play}
\label{sec:reward}
To enable free play in the agent, we propose an intrinsic reward that encourages the agent to explore its environment in a semantically expressive manner, akin to free play in humans.
This section describes the mathematical formulation of this semantics entropy reward and the additional regularity reward that complements it.

\subsection{Semantics Entropy Reward}
\label{sec:semantics-reward}
% - Entropy reward and its regularization hyperparameters derived from the ZEST formulation

Mathematically, in its simplest form, it is formulated as the negative entropy of the likelihood of the predictions of the VLM for an image observation \(\bfo\) over a set of creative possibilities \(\bmc\).
We call this the \textit{semantics entropy reward},
\begin{equation}
    \label{eq:entropy-reward}
    r_{\text{semantics}}(\bfs, \bfo; \bmc, T) := -\cH(p(\bmc; \bfo, T)) = \sum_{\bfc \in \bmc} p(\bfc; \bfo, \bmc, T) \log p(\bfc; \bfo, \bmc, T).
\end{equation}

To improve the reward quality, we further extend this formulation by adopting a combination of suggestions from other studies on VLMs as a source of goal-conditioned rewards.

\cite{zest} developed ZeST (zero-shot task specification) in which they introduced \textit{delta embeddings} that use the difference in embeddings between the desired and baseline configurations to project out irrelevant CLIP-embedding-space dimensions, for both image and text inputs.
Given the natural language descriptions \(\bfg, \bfb \in \cL\) of the goal and baseline \(\bfo_0 \in \cI\) respectively, they defined the goal-baseline regularized reward function as the similarity between the delta embeddings,
\begin{equation}
    \label{eq:zest}
    r_{\text{zest}}(\bfs, \bfo; \bfo_0, \bfg, \bfb) = S_c((\psi_I(\bfo)-\psi_I(\bfo_0)), (\psi_L(\bfg)-\psi_L(\bfb))),
\end{equation}
where \(\bfo_t \in \cI\) is the image observation at time \(t\).
This proved to improve performance in language-guided goal-conditioned tasks by providing a more focused reward signal.
\cite{negprompt} further proposed a negative prompt formulation that used the negations of goal states instead of descriptions of the initial states as baselines.

Similarly, \cite{vlmrm}, in their \textit{VLM-RM} method, proposed a comparable goal-baseline regularization formulation, but also considered the projection of the image observation onto the direction from the baseline towards the goal in the CLIP embedding space;
\begin{equation}
    \label{eq:vlmrm}
    r_{\text{goal-reg}}(\bfs, \bfo; \bfg, \bfb) = 1 - \frac {1}{2}||\alpha\cdot \text{proj}_{L}\psi_I(\bfo) + (1 - \alpha)\cdot (\psi_I(\bfo) -\psi_L(\bfg))||^{2}_{2},
\end{equation}
where \(L\) is the line spanned by \(\psi_I(\bfb)\) and \(\psi_L(\bfg)\), and \(\alpha \in [0, 1]\) is a parameter to control the regularization strength.

For \(\alpha = 0\), the original CLIP similarity metric from eq. \ref{eq:clip-similarity} is recovered as an unregularized goal-conditioned reward function.
On the other hand, for \(\alpha = 1\), the projection removes all components of \(\bfo\) orthogonal to "\(\bfg - \bfb\)".
Intuitively, the direction from \(\bfb\) to \(\bfg\) captures the change from the environment's baseline to the target state; by projecting the reward in this direction, irrelevant parts of the CLIP representation are directionally removed.
However, it is not certain that the direction really captures all the relevant information.
Therefore, instead of using \(\alpha = 1\), it is treated as a hyperparameter.
The authors found this method to be relatively robust to changes in \(\alpha\) with most intermediate values being better than \(0\) or \(1\).

We simplify and combine these ideas to formulate a baseline regularized semantics entropy reward,
\begin{equation}
    \label{eq:entropy-reward-reg}
    r_{\text{semantics-reg}}(\bfs, \bfo; \bfo_0, \bfg, \bfb, T) = -\cH(p'((\bmc, \bfb, \alpha); (\bfo, \bfo_0, \beta), T)),
\end{equation}
where \(\alpha, \beta \in [0, 0.5]\) are the regularization hyperparameters, and \(p'\) is the likelihood of the predictions of the VLM for the image observation \(\bfo\) given the image baseline \(\bfo_0\) over the set of creative possibilities \(\bmc\) and the text baseline \(\bfb\), which could either be the negation of the goal or the initial state description.

Although CLIP ideally abstracts away frivolous phrasing to focus on the semantics of a prompt, its phrasing has been shown to influence its inference \citep{waffleclip}. 
We experimented with this in our analysis.
Thus, for objective analysis, the phrasing for a language prompt \(\bfc \in \cL\) to represent a creative possibility \(c\) is broken down into \("\{\text{prefix}\}\{c\}\{\text{suffix}\}"\), where "prefix" and "suffix" are optional context phrases.

\subsection{Regularity Reward}
\label{sec:regularity-reward}
In addition to the entropy reward, the semantic reward also consists of regularity as an intrinsic reward \citep{rair}, which encourages the agent to create regular and symmetric patterns in its creations, which could promote semantic expression.
The authors described the regularity reward \textit{RaIR} as "(the) redundancy in the description of the situation, to measure the degree of sub-structure recurrence".

Mathematically, it is considered that the state space can be factorized into a composition of the different entities in the environment and the configuration of the agent, \(\cS = (\cS_{\text{obj}})^N \times \cS_{\text{agent}}\), where \(\cS_{\text{obj}} \in \nR^d\), such that \(n_s = N \cdot d\).
This allows \textit{RaIR} to be formulated in terms of multiplicities \(m: \cX \mapsto \nN\) (counts of occurrence) of the sub-structures \(\cX\) in the arrangement of the different entities of the environment in the state \(\bfs\),
\begin{equation}
    \label{eq:rair}
    r_{\text{RaIR}}(\bfs, \bfo) := -\cH(\phi(\bfs)) = \sum_{\cX \in \phi(\bfs)} p(\cX) \log p(\cX),
\end{equation}
where \(\phi\) is a factorization function that decomposes a state \(\bfs\) into a multiset of its unique sub-structures,
\begin{equation}
    \label{eq:rair-factorization}
    \phi : (\cS_{obj})^{N} \mapsto \{ \cX_1^{m(\cX_1)}, \cX_2^{m(\cX_2)}, \ldots \},
\end{equation}
and \(p(\cX)\) is the probability of a sub-structure \(\cX\) in the state \(\bfs\) calculated as,
\begin{equation}
    \label{eq:rair-probability}
    p(\cX) = \frac{m(\cX)}{\sum_{\cX' \in \phi(\bfs)} m(\cX')}.
\end{equation}
% 
For our purposes, following insights from \cite{symmetry} and \cite{compositional}, we use the relational formulation of \textit{RaIR} which captures the pairwise relationships between the entities of the environment in the state \(\bfs\) by calculating the distances between them in the discretized coordinate space;
\begin{equation}
    \label{eq:rair-relational}
    \phi(\bfs) = \{ \cX^{m(\cX)} : \cX = \lfloor s^{(j)} - s^{(k)} \rceil \mid s^{(j)}, s^{(k)} \in \bfs \},
\end{equation}
where the \(\lfloor\cdot\rceil\) operator groups the distances into discrete bins based on a configured resolution.

\section{iCEM}
\label{sec:icem}
% The Cross-Entropy Method (CEM) is an adaptive importance sampling procedure.

In the Model-Based Reinforcement Learning (MBRL) paradigm, the agent learns a model \(f\) of the MDP dynamics from interactions with the environment and uses it to predict the consequences of its actions which enables it to plan for the best policies.
Such policies can be learned with universal function approximators such as neural networks using RL algorithms or planning methods.

The improved Cross-Entropy Method (iCEM) is one such planning method.
It is a zeroth-order Monte-Carlo trajectory optimizer that works in a Model Predictive Control (MPC) loop to solve finite-horizon planning problems, i.e. it iteratively re-plans after every step in the environment by (1) coming up with several action sequences (by sampling a multi-variate distribution) to 'think' a few steps ahead, (2) imagining/predicting the future states resulting from these actions (using the model of the environment) and assessing their expected returns (using the reward function), (3) picking out the best action sequences, (4) improving the 'thinking' method based on these elite sequences, and (5) taking the first step of the best action sequence.

Concretely, at every timestep \(t\), there are multiple iterations of the following calculations. At iteration \(i\),
(1) given a starting sampling distribution \(p\) over action sequences (\(\bma = \{ \bfa_0, \bfa_1, \ldots, \bfa_{h-1} \}\)) in a finite horizon \(h\), a fixed-length (\(n\)) set of action sequences (\(\cA_t = \{ \bma_1, \bma_2, \ldots, \bma_n \}\)) is sampled from the distribution.
These samples are augmented with a fraction of the best action sequences from the previous iteration (called the \textit{elite-set} \(\cE_t^{(i-1)}\)).

(2) Every action sequence in this augmented set \(\cA'_t\) is then rolled out (in "imagination") using the environment model \(f\) and rendering function \(\theta\) from the current state \(\bfs_t\) to predict their corresponding expected states and image observations respectively,
\vspace{-1.5pt}
\begin{align}
    \cS_t &= \{ \{ \bfs_{t+1}, \bfs_{t+2}, \ldots, \bfs_{t+h} \} \}_{j=1}^{n}, \label{eq:icem-rollout}\\
    \cO_t &= \{ \{ \bfo_{t+1}, \bfo_{t+2}, \ldots, \bfo_{t+h} \} \}_{j=1}^{n}, \label{eq:icem-observations}
    \intertext{where \(\bfs_t = f(\bfs_{t-1}, \bfa_{t-1})\) and \(\bfo_t = \theta(\bfs_t)\). The predicted states are evaluated according to the reward function \(r\) to get their corresponding rewards,}
    \cR_t &= \{ \{ R_{t+1}, R_{t+2}, \ldots, R_{t+h} \} \}_{j=1}^{n}, \label{eq:icem-rewards}
\end{align}
where \(R_{t} = r(\bfs_t, \bfo_t)\). The corresponding trajectories are,
\begin{equation}
    \label{eq:icem-trajectories}
    \cT_t = \{\{(\bfs_t, \bfo_t, R_t, \bfa_t), \ldots, (\bfs_{t+h-1}, \bfo_{t+h-1}, R_{t+h-1}, \bfa_{t+h-1}), (\bfs_{t+h}, \bfo_{t+h}, R_{t+h}, \emptyset) \}\}_{j=1}^{n}.
\end{equation}
% 
(3) Subsequently, a new elite-set of actions \(\cE_t^{(i)}\) of size \(K\) is determined; it consists of the best \(K\) action sequence samples that maximize an aggregation of the returns from their expected states according to a reward aggregation function \(g\);
\begin{equation}
    \label{eq:icem-filter}
    \cE_t^{(i)} = \underset{\text{Best \(K\) action sequences in the augmented set of samples \(\cA'_t\)}}{\{ \bma_k \in \cA'_t : \ \mid \bma_j \in \cA'_t : g(\bmtau_k) \leq g(\bmtau_j) \mid\ \leq K \}}.
\end{equation}
More information about the reward aggregation strategies used in our experiments is provided in \autoref{sec:reward-aggregation}.

(4) These new elite action sequences are finally used to refine the distribution to maximize the expected return of its samples.
\vspace{-1.5pt}
\begin{align}
    \label{eq:icem-update}
    \bmmu_{t}^{(i+1)} &= \alpha\bmmu_t^{(i)} + (1-\alpha)\bmmu^{\cE_t^{(i)}},\\
    (\bmsigma_{t}^{(i+1)})^2 &= \alpha(\bmsigma_t^{(i)})^2 + (1-\alpha)(\bmsigma^{\cE_t^{(i)}})^2,
\end{align}
where \(\alpha \in [0, 1]\), and \(\bmmu_{t}^{(i+1)} \in \nR^{n_a \times h}\) is the mean and \(\bmsigma_{t}^{(i+1)} \in \nR^{n_a \times h}\) is the standard deviation of the sampling distribution at time \(t\). Specifically, it is a clipped colored-noise Gaussian distribution given by,
\begin{equation}
    \label{eq:icem-distribution}
    p(\bma) = \text{clip}(\bmmu_t + \cC^\beta(n_a, h) \odot \bmsigma_t^2),
\end{equation}
where \(\cC^\beta(n_a, h)\) is a sampling function that returns \(n_a\) (one for each action dimension) sequences of length \(h\) sampled from colored noise normal distribution with exponent \(\beta\) (\(= 1\) in our experiments; pink noise), zero mean and unit variance. 
At the end of this process, the above steps are repeated for a fixed number of \textit{inner} iterations for the same timestep.

After all \textit{inner} iterations are completed, the distribution concentrates on action sequences with high returns, and the best action sequence converges to an optimum;
\begin{equation}
    \label{eq:planning-goal}
    \bma^* = \argmax_{\bma_k \in \cA'} g(\bmtau_k).
\end{equation}
(5) Finally, the first action of the best elite sequence of actions \(\bma^*_t\) is executed in the environment.
The process is repeated for a specified number of steps or until a terminal state or reward/cost threshold is reached.

In our experiments, the sparse-reward setting and computational bottleneck due to the comparatively long inference times for CLIP made iCEM a suitable choice.
The colored noise of the iCEM distribution leads to temporally correlated action sequences over the horizon for smoother trajectories which is helpful for sparse-reward scenarios.
Furthermore, the addition of memory gives it good sample efficiency, which was essential for our case.

Since we are using a zero-order trajectory optimizer with a limited sample budget and finite-horizon planning, we don't necessarily converge to the global optima.
Although this does not solve the full reinforcement learning problem (infinite horizon and stochastic environments), it is very powerful in optimizing for tasks ad-lib without further adaptation, which makes it suitable for optimizing changing exploration targets.

For this to be successful, a good transition model \(f\) needs to be learned, and the reward function needs to be known or discovered.
In our case, the reward function is well-defined, and to evaluate the efficacy of our entropy minimization objective in achieving good semantic expression,
we do planning using ground truth (GT) models, i.e. with access to the true simulator of the environment itself for planning.
Thus, we can perform multi-horizon planning without any accumulating model errors using ground truth models, which allows us to better investigate the global/local optima of our semantics reward. 

\subsection{Reward Aggregation Strategies}
\label{sec:reward-aggregation}
There are many different ways to aggregate the rewards of the trajectory samples over the horizon.
This is treated as one of the hyperparameters in our experiments.
\vspace{-1.5pt}
\begin{align}
\intertext{A trivial reward aggregation function is the sum of the rewards over the horizon (\texttt{sum}),}
\texttt{sum}(\bmtau) &= \sum_{k=1}^{h} R_k. \label{eq:reward-aggregation-sum}
\intertext{We further experimented with a discount factor \(\gamma \in \nR^+\) over the horizon, which is treated as an additional hyperparameter,}
\texttt{sum-\textit{\(\gamma\)}}(\bmtau) &= \sum_{k=1}^{h} \gamma^{k-1} R_k. \label{eq:reward-aggregation-sum-discount}
\intertext{However, this type of consolidating aggregation strategy is not suitable for cases where an increase in return can be preceded by an initial decrease (as a consequence of leaving local optima). So we also considered the \texttt{best} method, which uses the best return over the planning horizon;}
\texttt{best}(\bmtau) &= \max_{k=1}^{h} R_k. \label{eq:reward-aggregation-best}
\intertext{Additionally, to combine the better qualities of both the \texttt{sum} and \texttt{best} methods; we formulated a dynamic reward aggregation method that ranks trajectories based on the highest sum of consecutive rewards in a window of a predefined (hyperparamterized) length \(l\), \texttt{best-\textit{l}};}
\texttt{best-\textit{l}}(\bmtau) &= \max_{k=1}^{h-l+1} \sum_{j=k}^{k+l-1} R_{k+j}. \label{eq:reward-aggregation-best-dynamic}
\intertext{Another strategy is to assess the trajectories based on their last states (\texttt{last}), i.e. to simply consider only what the agent could potentially achieve at the end of the horizon.
This is particularly useful when inferring the reward is expensive, as in our case, because only one final state needs to be evaluated;}
\texttt{last}(\bmtau) &= R_h. \label{eq:reward-aggregation-last}
\intertext{Finally, we also experimented with a combination of the \texttt{sum} and \texttt{last} methods with \texttt{last-\textit{l}}, which is the sum of the returns of the last \(l\) steps of the trajectory;}
\texttt{last-\textit{l}}(\bmtau) &= \sum_{k=h-l+1}^{h} R_k. \label{eq:reward-aggregation-sum-last}
\end{align}
