% The Experiments of the study should be laid out in a series of declarative paragraphs. Only results essential to establish the main points of the work should be included. Often the reporting of the results can be clearer if broken down into subsections. All figures and tables must be cited in the text and must be numbered in the order of their text citation. Figure legends should be self-explanatory, without referring to the text. They should identify the material that is being illustrated, what is shown, and its significance. Each table should be identified by number and should have a title. This section should not include long passages about the rationale of the experiments (which belong in the Introduction), or the methods used (which belong in the Material and Methods), nor should it include justification or discussion of the results (which belong in the Discussion section).
\chapter{Experiments}
\label{sec:experiments}

We perform these experiments to showcase that we can indeed get semantically expressive creations with our proposed formulation.

\section{Environments}
\label{sec:environments}
We use two custom environments in our experiments which allow for rich creative expression.

\begin{figure}[H]
    \centering
    \subfloat[\centering ShapeGridWorld]{\fbox{\includegraphics[width=0.4\textwidth]{images/sgw_random.png}}}
    \qquad
    \subfloat[\centering Tangram]{\fbox{\includegraphics[width=0.4\textwidth]{images/tangram_random.png}}}
    \caption{The environments.}
    \label{fig:environments}
\end{figure}

\subsection{ShapeGridWorld}
\label{sec:sgw}
ShapeGridWorld is a pixel grid environment where the agent can draw on the grid by moving pixels around, either one or all of them at the same time.
This is reminiscent of the pin-board platform used in the free play study conducted by \citet{diggs} (\figref{fig:diggs}).
By setting the pixels in a specific layout, the agent can draw a variety of shapes on the grid.
This environment was adapted from the pixel grid environment used in the work by \citet{rair} with some modifications summarised in appendix \ref{sec:sgw-details}.

ShapeGridWorld was mostly used in the initial experiments but it later proved to promote noise in CLIP inferences which led to underwhelming results.
To work around the problems, the Tangram environment was developed.

\subsection{Tangram}
\label{sec:tangram}
Tangram is a traditional puzzle game consisting of a canvas and seven geometric shapes -- five right isosceles triangles (two large, one medium, and two small), one square, and one parallelogram, which conventionally have distinct colors.
These shapes can be rotated, flipped, and translated on the canvas, without overlapping.
Even though this is a very simple setup, these pieces can be arranged in very different configurations to create expressive abstract patterns.

The Tangram virtual environment was developed specifically for our experiments as it mitigated the problems discussed in sections \secref{sec:sparse-rewards} and \secref{sec:inference-noise}, and is a major contribution of our work.

We mostly configured the environment to correspond with the traditional Tangram game.
The shapes involved were the same, no overlap was allowed, and the shapes could be rotated, flipped, and translated.
Yet, we did experiments with colors to improve the performance of CLIP and also gave the agent the freedom to use a subset of the shapes to make its creations.
Please refer to appendix \ref{sec:tangram-details} for more technical details about this environment.

\section{Initial Tests with CLIP}
\label{sec:clip-custom}

% Talk about the initial tests with CLIP that were motivated by the poor performance of CLIP on abstract images and sketches.
CLIP was trained on a large dataset of captioned natural images foraged from the internet.
While it generalizes well to many natural-image distributions and has proved to be a powerful zero-shot model for various tasks, it was not clear how well it would perform on sketches and abstract images. 

CLIP has been shown to not perform much better than chance on some tasks as they were not well represented in its pre-training dataset.
% For example, it only achieved 88\% accuracy on the handwritten digits of MNIST.
The authors of CLIP also noted that it is particularly poor on tasks for fine-grained classification such as differentiating models of cars, species of flowers, and variants of aircraft.
It also struggles with more abstract and systematic tasks such as counting the number of objects in an image.

Moreover, \cite{vlmrm}, from their experiments with CLIP as a source of goal-conditioned rewards, reported that CLIP rewards are only meaningful and well-shaped for environments that are photorealistic enough for the CLIP visual encoder to interpret correctly.
They found that it is crucial to add textures and shading to the images to make them more realistic for CLIP to perform well.

Influenced by these observations and caveats, we suspected that our environments, since they are not photorealistic, might also be out-of-distribution. 
Yet, we expected them to be simple enough for CLIP to interpret correctly.

Thus, before testing our reward function, we conducted a series of simple experiments on the inference capabilities of CLIP on ShapeGridWorld and Tangram.
At the same time, there were several hyperparameters in the environments and CLIP into which we took insights with these initial tests.


\subsection{CLIP on Sketches and ShapeGridWorld}
\label{sec:clip-sketches}

We first tested CLIP on sketches of simple shapes from the ImageNet-Sketch dataset \citep{imagenet}.

We also compared these results with renderings of these sketches on ShapeGridWorld grids of different resolutions by registering them (see \secref{sec:sgw-registration} for more details).

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{images/grid_comparison.pdf}
    \includegraphics[width=\textwidth]{images/grid_comparison_images.png}
    \includegraphics[width=\textwidth]{images/grid_comparison_accuracy.pdf}
    \caption{CLIP on sketches and ShapeGridWorld grids of different resolutions.}
    \label{fig:clip-sketches}
    % \includegraphics[width=\textwidth]{images/grid_comparison_inverted.pdf}
    % \includegraphics[width=\textwidth]{images/grid_comparison_accuracy_inverted.pdf}
    % \caption{CLIP on \emph{inverted} sketches and ShapeGridWorld grids of different resolutions.}
    % \label{fig:clip-sketches-inv}
\end{figure}

As an example, \figref{fig:clip-sketches} shows the comparative similarity of the embeddings of \(80\) sketches of apples (and their grid counterparts) to the label ``sketch of an apple'' using the CLIP variant \texttt{ViT-L/14}, as a probability.
The probabilities are calculated using the equation \eqref{eq:clip-dist}, with temperature \(T = 1\), to show the trend.
The categories for this example are ``apple'', ``chair'', ``car'', ``flower'', ``pencil'', ``house'', ``tree'', ``fish'', ``star'', and ``bird''.
The middle row shows a sample of the images for each of the cases.
The bottom row shows the accuracy (percentage of correct predictions) for each of the cases.

\begin{wrapfigure}{r}{0.4\textwidth}
    \centering
    \includegraphics[width=0.36\textwidth]{images/hypercategory_comparison_2.pdf}
    \caption{CLIP on different specificity of labels.}
    \label{fig:clip-hypercategory}
\end{wrapfigure}

We found that CLIP can recognize these sketches quite well.
As the grid became coarser, the confidence and accuracy of CLIP decreased significantly.
For very coarse grids, the accuracy is almost as bad as random guessing.
We also experimented with inverting the images and found that results were consistently slightly better with black-on-white sketches (shown here) than with white-on-black sketches.
Yet, we did not notice any difference in performance with the addition of grayscale values in the pixel blocks.
In most cases, we also noticed that using specific labels like ``apple'' for a picture of an apple had better accuracy than using hypernyms like ``fruit'' (\figref{fig:clip-hypercategory}).

Additionally, we found that the bigger CLIP models performed significantly better on these tasks than the smaller ones which is in line with observations from other studies on VLMs as a source of rewards.
Furthermore, we tried the same experiments with different sets of categories, prefixes, and suffixes, but the results followed similar trends.
The later sections give more results for experiments on these hyperparameters.


\subsection{CLIP on Tangram}
\label{sec:clip-tangram}

Tangram has lesser degrees of freedom than ShapeGridWorld and it is more abstract but it still allows for rich creative expression.
To test if it would be a feasible environment for our study, we used some images of creations on Tangram from the internet and used CLIP as a zero-shot classifier as above to test its performance.
We again found it to be quite good at recognizing these creations with very high confidence.
\figref{fig:clip-tangram} shows a few examples.
\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{images/tangram_comparison_10.pdf}
    \caption{CLIP on a few simple Tangram creations.}
    \label{fig:clip-tangram}
\end{figure}

We did not find a significant trend in the effect of colors on the performance of CLIP, but inversions seemed to slightly affect the distribution of inferences in certain cases
(one such example is given in the appendix \figref{fig:clip-tangram-inversions}, where the distribution of CLIP was skewed in grayscale and white-on-black cases).
Consequently, we used colored or black-on-white binary renderings for our experiments.

More experiments on these hyperparameters are discussed in the later sections.


\section{Trajectory Analysis with Random Rollouts}
\label{sec:clip-problems}
% Problems with CLIP
Although we found that CLIP is quite good at recognizing these creations, we had to ensure that it would be a good source of rewards for our environments, or that the controller would be able to exploit it well to reach a sufficiently good local optimum.
To study the semantics entropy reward landscape, we conducted a series of experiments with random rollouts in the environments, i.e. starting from a meaningful creation, we perturbed the creation with randomly sampled actions and observed the effects on the reward.

These experiments showed that CLIP is quite susceptible to noise in its inferences, i.e. its probability distribution is very sensitive to small adjustments in the input image.
As a consequence of this, the reward is quite sparse, and more critically, there is a large semantic bias in not-so-meaningful images.
The sections below discuss these problems in detail.

\subsection{Sparse Rewards} % Sudden changes in rewards
\label{sec:sparse-rewards}

We observed that the inference of CLIP breaks suddenly with small changes in the image, i.e. its confidence in its classification changes very abruptly.
This results in a sparse semantics entropy reward landscape.
\figref{fig:sparse-rewards} illustrates this problem.
We destroy the image pixel by pixel using random actions left to right and see how CLIP's output changes.

\begin{figure}[h]
    \centering
    \missingfigure{Random rollouts in ShapeGridWorld.}
    \caption{Random rollouts in ShapeGridWorld.}
    \label{fig:sparse-rewards}
\end{figure}

Sparse rewards are a problem in reinforcement learning in general because underly a lack of continuous feedback. 
With only sporadic rewards, it becomes exceedingly difficult for an agent to gauge the values of actions to lead it to eventual rewards, i.e. credit assignment becomes difficult, which in turn leads to difficult and inefficient learning of action policies.

% If we imagine this scenario in reverse with an active controller, i.e. if the agent starts from the random image and tries to converge to a meaningful image by relying on the semantics entropy reward, it will have to take a series of actions that would sometimes lead to a reward and other times not.

Thus, a smooth and continuous reward is essential for effective learning.
Since we used the iCEM controller which is a gradient-free optimization algorithm and has proved to work well in sparse reward settings, we were able to circumvent this problem.

\subsection{Semantic Bias in Random Images} % Confidence in random images
\label{sec:inference-noise}
The other consequence of the noise in CLIP inference is that can sometimes confidently classify random images to a class, i.e. false positives, instead of having a flat distribution.
\figref{fig:semantic-bias-random} shows some instances of this problem where CLIP displayed high confidence in random images.

\begin{figure}[h]
    \centering
    % \includegraphics[width=0.6\textwidth]{images/p_random_semantic_bias.png}
    \includegraphics[width=\textwidth]{images/adversarial_samples.pdf}
    \caption{Large semantic bias in random images.}
    \label{fig:semantic-bias-random}
\end{figure}

This can also be interpreted as a jagged reward landscape with many local optima.
This noisy reward signal can lead the agent to be stuck in local optima (with a rather meaningless creation that it erroneously finds confident), or inhibit it from reaching a good optimum.

The two problems, sparse rewards and semantic bias in random images are not independent.
They have the same underlying cause -- noisy CLIP inference, yet there's a non-trivial anticorrelation between reward sparseness and noise.
That is, increasing reward density involves increasing confidence in imperfect/partial creations which also increases noise.


\subsubsection{Class Preference in CLIP}
\label{sec:class-preference}
Another complementary problem we faced related to the problem of semantic bias in random images was that of class imbalance.
Simply put, there were some classes in which CLIP was consistently more confident than others and leaned towards them over others when unsure.
This included classes that signified broad concepts like ``animal'', ``fruit'', ``number'', ``letter'', or ``object''.

% \begin{figure}[H]
%     \centering
%     \includegraphics[width=\textwidth]{images/inference_noise.png}
%     \caption{Class preference in CLIP.}
%     \label{fig:class-preference}
% \end{figure}

This made choosing the right categories for the semantics entropy reward such that CLIP did not have a strong preference for any of them quite important.
More discussion on tackling this problem in particular is given in section \secref{sec:clip-categories}.


\section{Trajectory Analysis with Closeness Costs}
\label{sec:closeness-rollouts}
% closeness_reward_scale
% closeness_reward_threshold
Before we started improving and running simulations using inferences from CLIP, we needed to ensure that the controller was able to reach a reward-conditioned goal in our environments, i.e. it was \emph{solvable}.
Especially, it was crucial to establish this solvability under sparse rewards.

To ensure that our controller solved the environment efficiently and robustly, a good choice of the iCEM controller hyperparameters and the environment configuration was essential.
These hyperparameters and configurations were dependent on each other so we needed to optimize them together.

For example, for both environments, the size of the grid together with the step size affects the minimum required planning horizon for the controller.
For a small discrete grid or a large enough step size, the planning horizon does not need to be very high, because the controller can potentially reach the goal in a few steps and does not need to plan far ahead.
If the step size is too small for the grid size, a longer planning horizon is required so that the agent can discover the reward by random sampling.

In a similar vein, the step size is also related to the \emph{object persistency} used in the environments.
This parameter governs how many steps a single object (pixel block in ShapeGridWorld and a polygon in Tangram) is in focus for the actions of the controller before it cycles to the next object in the sequence.
For a high object persistency, the step size could be lower, but for a low object persistency, the step size needs to be higher.
(Please refer to appendix \ref{sec:icem-hyperparameters} and \ref{sec:env-hyperparameters} for more details about the hyperparameters of the iCEM controller and the environments.)

These interdependencies made the hyperparameter optimization problem quite complex and the sheer number of these parameters made this task very expensive.
Given the high computation resources and time required to run simulations with the semantics entropy reward using CLIP, it was infeasible to do a proper search over all the hyperparameters.
Thus, we instead used an alternative reward function to analyze the effect of the hyperparameters.
We called this ersatz reward function the \emph{closeness reward}.

A closeness reward is defined in the context of a fixed target creation in the environment, and its function is given as the negative of a distance function between the current creation and the target creation in the state space of the environment.

We used two different formulations of the distance function, which we called the \emph{dense closeness cost} and \emph{sparse incremental closeness cost}.
The dense closeness cost is formulated as the \(L^2\) distance between the current and target creations and the sparse incremental closeness cost is formulated as the dimension-wise thresholded \(L^1\) distance between the current and target creations, given by,
\begin{equation}
    \label{eq:closeness-reward-sparse}
    d(\bfi, \bft) = \sum_{k \in n_s} \bm{1}_\varepsilon(\bfi_k - \bft_k),
\end{equation}
where \(\bfi, \bft \in \cS\) are the current and target creations respectively, \(\varepsilon\) is a small threshold, and \(\bm{1}_\varepsilon\) is the indicator function such that \(\bm{1}_\varepsilon(x) = 1\) if \(|x| > \varepsilon\) and \(0\) otherwise.

Using the closeness reward/cost formulations, we performed an exhaustive grid search over the many hyperparameters of the iCEM controller and the environment together to find the best combinations.
The controller was able to achieve the goal under dense closeness rewards for a wide range of hyperparameters, but the tuning was more difficult under sparse incremental rewards.
Yet, we were able to find the minimal set of hyperparameters that allowed the controller to solve the environment under sparse rewards.

\begin{figure}[H]
    \centering
    \missingfigure{Closeness rollouts in Tangram.}
    \caption{Closeness rollouts in Tangram.}
    \label{fig:closeness-rollouts}
\end{figure}

To assess and compare the results of different parameters in these experiments we compared their cumulative rewards over the rollouts.

The results for the sparse incremental closeness cost on the Tangram environment are shown in \figref{fig:closeness-rollouts}.

\subsubsection{Tuning Environment Parameters}
\label{sec:env-hyperparameters}
% render_kwargs.invert
% render_kwargs.color

\label{sec:sgw-hyperparameters}
% width
% x_step
% render_delta
% object_persistency
% max_dist
% control
% control_boundaries


\label{sec:tangram-hyperparameters}
% flip
% rotate
% x_size
% r_size
% x_step
% object_persistency
% max_dist
% control
% control_boundaries
% staging_boundaries


\subsubsection{Tuning Controller Hyperparameters}
\label{sec:icem-hyperparameters}
% action_sampler_params.opt_iterations
% action_sampler_params.init_std
% action_sampler_params.elites_size
% num_simulated_trajectories
% horizon
% cost_along_trajectory
% discount_along_trajectory

\todo{Talk about the different hyperparameters that were considered. Show histograms of only the significant parameters.}


\section{Improving CLIP Rewards}
\label{sec:improving-rewards}

We experimented with several methods to reduce the noise from CLIP.
The regularized semantics reward also has many hyperparameters that can be tuned to make the reward landscape smoother.
Namely, the choice of categories/creative possibilities (\(c\)), the prefix and suffix to these categories, the temperature of the softmax function (\(T\)), the target baseline regularization strength (\(\alpha\)), the image baseline regularization strength (\(\beta\)), use of negative embeddings, and tweaking the rendering function (adding texturing or modifying the images with other operations).

The complex nature of the reward landscape and the high dimensionality of the state space made it difficult to analyze the effects of these hyperparameters.
To gain an intuition over these parameters and reduce the search space before we ran simulations using them in the full semantics reward, we instead used the best of the previously rolled-out trajectories with the closeness reward from \secref{sec:closeness-rollouts} and ran post hoc inference on the resulting sequence of image observations using CLIP and calculated the trajectories of the resulting semantics reward.
Subsequently, we performed an ablation study on these hyperparameters to gain insights into their effects \footnote{The myriad of combinations due to the high dimensional space makes it infeasible to show all the subtleties of the interplay between these hyperparameters in a few figures.
Thus, we have additionally made the analysis available for the reader as an interactive notebook at \url{https://colab.research.google.com/drive/1UzKb5t5PDRO05GbSKpgzWd3DELfAzDxJ} or \url{https://t.ly/j84on}, where one can pick a combination of different hyperparameters to see their effects}.

The following sections discuss the results of these analyses.

\subsection{Effect of the Number of Creative Possibilities}
\label{sec:clip-categories}
We found that the number of creative possibilities in the environment can also affect the performance of CLIP.
Too few categories can promote semantic bias and class preference in random images, but too many categories can exacerbate the problem of sparse rewards.
We found that the best results were obtained with a moderate number of categories (\(\sim 10\)).
This helped in mitigating the class preference in CLIP.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{images/category_comparison_tangram.pdf}
    \caption{Effect of the number of categories on CLIP inference distribution over a random image.}
    \label{fig:clip-categories}
\end{figure}

\subsection{Effect of Temperature}
\label{sec:reg-temperature}
% semantics_model_temperature
\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{images/temperature_comparison.pdf}
    \caption{Effect of temperature on semantics entropy reward trajectories.}
    \label{fig:clip-temperature}    
\end{figure}

\subsection{Effect of Target Baseline Regularization}
\label{sec:reg-alpha}
% semantics_alpha_target


\subsection{Effect of Image Baseline Regularization}
\label{sec:reg-beta}
% semantics_beta_image

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{images/alpha_beta_temp12avg_noneg.pdf}
    \caption{Effect of regularization strength on semantics entropy reward trajectories.}
    \label{fig:clip-alpha-beta}
\end{figure}


% \subsection{Effect of Changing Prefix and Suffix}
% \label{sec:prefix-suffix}
% % label_prefix
% % label_suffix
% Different prefixes/suffixes and combinations of them also affected the performance of CLIP.

% \begin{figure}[H]
%     \centering
%     \missingfigure{Effect of changing the prefix suffix.}
%     \caption{Effect of different prefixes and suffixes.}
%     \label{fig:prefix-suffix}
% \end{figure}

\subsection{Effect of Adding Post-Suffix}
\label{sec:post-suffix}
Following the insights from \cite{waffleclip}, we also experimented with adding concepts and/or a random string of jibberish to the end of the label as a \emph{post-}suffix to see if it affected the reward. 

While it did affect the reward, the effect was not significant, and we could not realize the exact mechanism for choosing a post-suffix that would work well in general.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{images/post_suffix_comparison.pdf}
    \caption{Effect of different post-suffixes on semantics entropy reward trajectories.}
    \label{fig:post-suffix}
\end{figure}


\subsection{Effect of Negative Embeddings}
\label{sec:negative-embeddings}
\cite{negprompt} used negative embeddings in their goal-conditioned CLIP reward function to make the reward landscape smoother.
We experimented with this as well, and instead of using the initial description of the environment as the text baseline, we used a negative embedding of the target creation.

We did not find this to be helpful in our experiments, instead it seemed to flatten the reward landscape.
We think this is a consequence of CLIP's language encoder being a bag-of-words model, and the negative embeddings being essentially close to the target embeddings, which effectively zeros out the target embeddings.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{images/baseline_comparison.pdf}
    \caption{Effect of different text baselines on semantics entropy reward trajectories.}
    \label{fig:baseline}
\end{figure}

\subsection{Effect of Image Texturing}
\label{sec:image-texturing}
Following the observations from \cite{vlmrm}, we also experimented with adding textures to the images to make them more realistic for CLIP in hopes of improving its quality.


\subsection{Effect of Image Operations}
\label{sec:image-operations}
% Shearing and Hatching
We also had the idea to use image operations like shearing and rotation before inference.
We expected the true positive semantic inferences to be invariant to these operations, but the false positives to be reduced.

While rotating did not have a significant effect, we found that shearing the images before inference did improve the quality of the inferences slightly.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{images/texturing_operations_comparison.pdf}
    \caption{Effect of texturing and image operations on semantics entropy reward trajectories.}
    \label{fig:texturing-operations}
\end{figure}

% \begin{figure}[H]
%     \centering
%     \missingfigure{As a consolidation of everything before, a bar plot with ablations.}
%     \caption{Ablations of the different methods to improve semantics entropy reward.}
%     \label{fig:clip-ablation}
% \end{figure}


\subsection{Entropy Regularization}
\label{sec:entropy-regularization}
Another promising way to make the reward landscape of CLIP smoother is to fine-tune it with an additional entropy regularization loss over its output.
This is given by,
\begin{equation}
    \label{eq:entropy-regularization}
    L(\bmi, \bml) = L_{\iota}(\bmi, \bml) \underbrace{- \sum_{\bfi_k \in \bmi}\sum_{\bfl_j \in \bml} p(\bfl_j; \bfi_k, \bml) \log p(\bfl_j; \bfi_k, \bml)}_{\text{Entropy Regularization}},\\
\end{equation}
where \(L_{\iota}\) is the contrastive cosine-similarity loss used to train CLIP and \(p(\bfi_k, \bml)\) is the classification probability distribution of image \(\bfi_k\) over the labels \(\bml\) predicted by CLIP.

We experimented with this regularization method on a toy convolutional neural network (CNN) for classifying handwritten digits, which we called \emph{Flatnet}.
We found it to be quite effective in smoothing the reward landscape; it seemed to relieve both of the problems from \secref{sec:clip-problems} -- the reward trajectory was smoother and we observed less semantic bias in random images.

Yet, we did not use it to fine-tune CLIP to limit the scope of the project given the limited time.
The results of this analysis can be found in appendix \ref{sec:flatnet}.

\subsection{Adversarial Performance}
\label{sec:adversarial-performance}
To tackle the problem of semantic bias in random images, we collected samples of the false positive image observations from our rollouts, called \emph{adversarial observations}, by filtering out the image observations with low entropy from all our runs and then manually removing the ones that were true positives.
Then, we searched for the semantics reward regularization hyperparameters that would decrease its specificity while maintaining its sensitivity, i.e. make CLIP less confident in the adversarial images while maintaining its inference for the truly semantically expressive images.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{images/temperature_adversarial.pdf}
    \caption{Effect of temperature on discerning true positives from false positives.}
    \label{fig:clip-temperature-adversarial}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{images/alpha_beta_adversarial.pdf}
    \caption{Effect of regularization strength on discerning true positives from false positives.}
    \label{fig:clip-alpha-beta-adversarial}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{images/post-suffix_adversarial.pdf}
    \caption{Effect of different post-suffixes on discerning true positives from false positives.}
    \label{fig:post-suffix-adversarial}
\end{figure}

% \begin{figure}[H]
%     \centering
%     \includegraphics[width=\textwidth]{images/subplots/rair_.pdf}
%     \includegraphics[width=\textwidth]{images/subplots/entropy_.pdf}
%     \includegraphics[trim=1.6cm 0cm 0cm 0cm, width=\textwidth]{images/subplots/distribution_.pdf}
%     \includegraphics[width=\textwidth]{images/subplots/snapshots_.pdf}
%     \caption{A sample rollout distribution plot.}
% \end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{images/baseline_adversarial.pdf}
    \caption{Effect of different text baselines on discerning true positives from false positives.}
    \label{fig:baseline-adversarial}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{images/texturing-operations_adversarial.pdf}
    \caption{Effect of image operations on discerning true positives from false positives.}
    \label{fig:texturing-operations-adversarial}
\end{figure}

% \figref{fig:adversarial-images} shows the results of our analysis.
% \begin{figure}[H]
%     \centering
%     \includegraphics[width=\textwidth]{images/p_adversarial_histogram.png}
%     \caption{Effect of different prefixes and suffixes.}
%     \label{fig:prefix-suffix-adversarial}
% \end{figure}

By choosing the right hyperparameters for the semantics reward, we were able to improve the predictive value of CLIP for our environments.

\newpage
\section{Simulations}
\label{sec:simulations}
% Manual ranking and entropy ranking

Once we had found suitable combinations of the controller and environment hyperparameters, and had developed an intuition of the semantics entropy reward hyperparameters, we finally ran simulations with the semantics reward to generate creations in the environments.

To evaluate these creations, in addition to the cumulative reward, we also asked two human evaluators to manually rank the final creations on a scale of \(1\) to \(5\) based on their apparent semantic expressiveness.

% \subsection{Evaluating Regularized Entropy Reward Formulation in Goal-Conditioned Tasks}
% \label{sec:evaluating-regularized-entropy}
% % alpha_target
% % beta_image


% What makes CLIP think that the creation is a certain class?
% - Apple with the red and the green/yellow triangle
% - Hammer with the parallelogram and the square
% - Heart with the red triangle pointing down
% - House with a triangle and the square



\subsection{Effect of RaIR}
\label{sec:effect-rair}
% compression_precision
% 1 / semantics_reward_scale

The additional structural bias helped with grounding and coalescing the creations in more human-recognizable forms and patterns.


\subsection{Partial Completions on ShapeGridWorld}
\label{sec:partial-completion}
