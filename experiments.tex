% The Experiments of the study should be laid out in a series of declarative paragraphs. Only results essential to establish the main points of the work should be included. Often the reporting of the results can be clearer if broken down into subsections. All figures and tables must be cited in the text and must be numbered in the order of their text citation. Figure legends should be self-explanatory, without referring to the text. They should identify the material that is being illustrated, what is shown, and its significance. Each table should be identified by number and should have a title. This section should not include long passages about the rationale of the experiments (which belong in the Introduction), or the methods used (which belong in the Material and Methods), nor should it include justification or discussion of the results (which belong in the Discussion section).
\chapter{Experiments}
\label{sec:experiments}

% We perform these experiments to showcase that we can indeed get semantically expressive creations with our proposed formulation.

\section{Initial Tests with CLIP}
\label{sec:clip-custom}

% Talk about the initial tests with CLIP that were motivated by the poor performance of CLIP on abstract images and sketches.
CLIP was trained on a large dataset of captioned natural images foraged from the internet.
While it generalizes well to many natural-image distributions and has proved to be a powerful zero-shot model for various tasks, it was not clear how well it would perform on sketches and abstract images. 

CLIP has been shown to not perform much better than chance on many tasks which were not well represented in its pre-training dataset.
% For example, it only achieved 88\% accuracy on the handwritten digits of MNIST.
The authors of CLIP also noted that it is particularly poor on tasks for fine-grained classification such as differentiating models of cars, species of flowers, and variants of aircraft.
It also struggles with more abstract and systematic tasks such as counting the number of objects in an image.

Moreover, \cite{vlmrm}, from their experiments with CLIP as a source of goal-conditioned rewards, reported that CLIP rewards are only meaningful and well-shaped for environments that are photorealistic enough for the CLIP visual encoder to interpret correctly.
They found that it is crucial to add textures and shading to the images to make them more realistic for CLIP to perform well.

Influenced by these observations and caveats, we suspected that our environments, since they are not photorealistic, might also be out-of-distribution. 
Yet, we expected them to be simple enough for CLIP to interpret correctly.

Thus, before testing our reward function, we conducted a series of simple experiments on the inference capabilities of CLIP on ShapeGridWorld and Tangram.
At the same time, there were several hyperparameters in the environments and CLIP into which we took insights with these initial tests.


\subsection{CLIP on Sketches and ShapeGridWorld}
\label{sec:clip-sketches}

We first tested CLIP on sketches of simple shapes from the ImageNet-Sketch dataset \citep{imagenet}.

We also compared these results with renderings of these sketches on ShapeGridWorld grids of different resolutions by registering them to a grid (see \secref{sec:sgw-registration} for more details on registration).
For an example, see \figref{fig:clip-sketches}.
\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{images/grid_comparison.pdf}
    \includegraphics[width=\textwidth]{images/grid_comparison_images.png}
    \includegraphics[width=\textwidth]{images/grid_comparison_accuracy.pdf}
    \caption[CLIP on sketches and ShapeGridWorld grids of different resolutions.]{CLIP on sketches and ShapeGridWorld grids of different resolutions.
    The first row shows the comparative similarity of the embeddings of \(80\) sketches of apples (and their grid counterparts) to the label ``sketch of an apple'' using the CLIP variant \texttt{ViT-L/14}.
    The probabilities are calculated using the equation \eqref{eq:clip-dist}, with temperature \(T = 1\), to show the trend.
    The categories for this example are ``apple'', ``chair'', ``car'', ``flower'', ``pencil'', ``house'', ``tree'', ``fish'', ``star'', and ``bird''.
    The middle row shows a sample of the images for each of the cases.
    The bottom row shows the accuracy (percentage of correct predictions) for each of the cases.
    }
    \label{fig:clip-sketches}
    % \includegraphics[width=\textwidth]{images/grid_comparison_inverted.pdf}
    % \includegraphics[width=\textwidth]{images/grid_comparison_accuracy_inverted.pdf}
    % \caption{CLIP on \emph{inverted} sketches and ShapeGridWorld grids of different resolutions.}
    % \label{fig:clip-sketches-inv}
\end{figure}
We found that CLIP can recognize these sketches quite well.
As the grid became coarser, the confidence and accuracy of CLIP decreased significantly.
For very coarse grids, the accuracy is almost as bad as random guessing.

We also experimented with inverting the images and found that results were consistently slightly better with black-on-white sketches (shown here) than with white-on-black sketches.
Yet, we did not notice any difference in performance with the addition of grayscale values in the pixel blocks in these experiments.

\begin{wrapfigure}[13]{r}{0.39\textwidth}
    \centering
    \includegraphics[width=0.36\textwidth]{images/hypercategory_comparison_2.pdf}
    \caption{CLIP on different levels of hypernymy and hyponymy of labels.}
    \label{fig:clip-hypercategory}
\end{wrapfigure}
In most cases, we also noticed that using specific labels like ``apple'' for a picture of an apple had better accuracy than using hypernyms like ``fruit'' (\figref{fig:clip-hypercategory}).

Additionally, we found that the bigger CLIP models performed better on these tasks than the smaller ones which is in line with observations from other studies on VLMs as a source of rewards, particularly \cite{vlmrm}.
We tried the same experiments with different sets of categories, prefixes, and suffixes; the results followed similar trends.

\subsection{CLIP on Tangram}
\label{sec:clip-tangram}

Tangram has fewer degrees of freedom than ShapeGridWorld and it is more abstract but it still allows for rich creative expression.
To test if it would be a feasible environment for our study, we used some images of creations on Tangram from the internet and used CLIP as a zero-shot classifier as above to test its performance using classes with simple Tangram creations.
We again found it to be quite good at recognizing these creations with good confidence.
\figref{fig:clip-tangram} shows a few examples.
\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{images/tangram_comparison_10.pdf}
    \caption{CLIP on a few simple Tangram creations.}
    \label{fig:clip-tangram}
\end{figure}

We did not find a significant trend in the effect of colors on the performance of CLIP, but inversions seemed to slightly affect the distribution of inferences in certain cases
(one such example is given in the appendix \figref{fig:clip-tangram-inversions}, where the distribution of CLIP was skewed in grayscale and white-on-black cases).
Consequently, we used colored or black-on-white binary renderings for our experiments.

More experiments on these hyperparameters are discussed in the later sections.


\section{Trajectory Analysis with Random Rollouts}
\label{sec:clip-problems}
% Problems with CLIP
Although we found that CLIP is quite good at recognizing these creations, we had to ensure that it would be a good source of rewards for our environments, or that the controller would be able to exploit it well to reach a sufficiently good local optimum.
To study the semantics entropy reward landscape, we conducted a series of experiments with random rollouts in the environments, i.e. starting from a meaningful creation, we perturbed the creation with randomly sampled actions and observed the effects on the reward.

These experiments showed that CLIP is quite susceptible to noise in its inferences, i.e. its probability distribution responds abruptly to small adjustments in the input image.
As a consequence of this, the reward is quite sparse, and more critically, there is a large semantic bias in not-so-meaningful images.
The sections below discuss these problems in detail.

\subsection{Sparse Rewards} % Sudden changes in rewards
\label{sec:sparse-rewards}

We observed that the inference of CLIP breaks suddenly with small changes in the image, i.e. its confidence in its classification is very sensitive.
This can be interpreted as a jagged reward landscape with many local optima.
\figref{fig:sparse-rewards} illustrates this problem.
It shows that changing a single pixel might lead to a sudden change in the distribution of CLIP's inferences.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{images/sparse_rewards.pdf}
    \caption[Random rollouts in ShapeGridWorld.]{Random rollouts in ShapeGridWorld. Going from left to right in time, we perturb the grid pixel-by-pixel and observe CLIP's confidence change. The bottom row shows the distribution of CLIP's inferences over the labels ``car'', ``apple'', ``fruit'', ``letter'', and ``number''. The graph in the top row shows the distribution's entropy. The first and last images in the bottom row are the initial and final images respectively.
    The middle image shows the image after which ``car'' was no longer the categorized class.}
    \label{fig:sparse-rewards}
    
\end{figure}

There is a lack of continuous incremental feedback in the reward signal, i.e. the reward landscape is noisy and sparse.
If we imagine this scenario in reverse with an active controller, i.e. if the agent starts from the random image and tries to converge to a meaningful image by relying on the semantics entropy reward, it might register a sudden burst of rewards for small changes in the image, but these rewards are just artifacts of the noise in CLIP's inferences.
This makes it difficult for the agent to gauge the values of actions to lead itself to eventual rewards, which in turn leads to difficult and inefficient learning of action policies.

Thus, a smooth and continuous reward is essential for effective learning.

This demanded some regularization techniques that would reduce the noise in CLIP, and a more robust optimization algorithm that could still work well.
Since we used the iCEM controller which is a gradient-free optimization algorithm and has proved to work well in sparse reward settings, it helped to mitigate this problem.
% Moreover, we typically used the \texttt{sum} cost aggregation function for iCEM to average out the noise in the reward signals over the planning horizon.

\subsection{Semantic Bias in Random Images} % Confidence in random images
\label{sec:inference-noise}
Another consequence of the noise in CLIP is that it often confidently classifies random images to a class, i.e. false positives, instead of having a flat distribution over the classes, as observed here with the class ``number''.

This noisy reward signal can lead the agent to be stuck in local optima (with a rather meaningless creation that it erroneously finds confident), or inhibit it from reaching a good optimum by hindering the policy learning.

The two problems, sparse rewards and semantic bias in random images are not independent.
They have the same underlying cause -- noisy CLIP inference, yet there's a non-trivial anti-correlation between reward sparseness and noise.
That is, increasing reward density (or reducing reward sparsity) involves increasing confidence in imperfect/partial creations (or adding more semantic bias in random images).


\subsubsection{Class Preference in CLIP}
\label{sec:class-preference}
Another complementary issue we faced related to the problem of semantic bias in random images was that of class imbalance.
There are some classes in which CLIP is consistently more confident than others and leans towards them over others when unsure.
This included classes that signified broad concepts like ``animal'', ``fruit'', ``number'', ``letter'', or ``object''.

Thus in free-play, it converges to these classes more often than others.
Histograms of the creations we observed in our simulations shown in \figref{fig:class-preference-tangram} and \figref{fig:class-preference-sgw} clearly show this bias.

% \begin{figure}[H]
%     \centering
%     \includegraphics[width=\textwidth]{images/inference_noise.png}
%     \caption{Class preference in CLIP.}
%     \label{fig:class-preference}
% \end{figure}

This is a fundamental nature of the CLIP model and we could only mitigate it by avoiding certain categories and choosing enough categories for the semantics entropy reward such that CLIP's preferences balanced out.
More discussion on tackling this problem in particular is given in section \secref{sec:clip-categories}.

% \newpage
\section{Trajectory Analysis with Closeness Costs}
\label{sec:closeness-rollouts}
% closeness_reward_scale
% closeness_reward_threshold
Before we started improving and running simulations using inferences from CLIP, we needed to ensure that the agent was able to reach a reward-conditioned goal in our environments, i.e. it was \emph{solvable}.
Especially, it was crucial to establish this solvability under sparse rewards.

To ensure that our controller did this efficiently and robustly, a good choice of the iCEM controller hyperparameters and the environment configuration was essential.
These hyperparameters and configurations were dependent on each other so we needed to optimize them together.

For example, for both environments, the size of the grid together with the step size affects the minimum required planning horizon for the controller.
For a small discrete grid or a large enough step size, the planning horizon does not need to be very high, because the controller can potentially reach the goal in a few steps and does not need to plan far ahead.
If the step size is too small for the grid size, a longer planning horizon is required so that the agent can discover the reward by random sampling.

In a similar vein, the step size is also related to the \emph{object persistency} used in the environments.
This parameter governs how many steps a single object (pixel block in ShapeGridWorld and a polygon in Tangram) is in focus for the actions of the controller before it cycles to the next object in the predefined sequence.
For a high object persistency, the step size could be lower, but for a low object persistency, the step size needs to be higher.

These interdependencies made the hyperparameter optimization problem quite complex and the sheer number of these parameters made this task very expensive.
Given the high computation resources and time required to run simulations with the semantics entropy reward using CLIP, it was infeasible to do a proper search over all the hyperparameters.
Thus, we instead used an alternative reward function to analyze the effect of the hyperparameters.
We called this ersatz reward function the \emph{closeness reward}.

A closeness reward is defined in the context of a fixed target creation in the environment, and it is formulated as the negative of a distance function between the current creation and the target creation in the state space of the environment.

We used two different distance functions, which we called the \emph{dense closeness cost} and \emph{sparse incremental closeness cost}.
The dense closeness cost is formulated as the \(L^2\) distance between the current and target creations and the sparse incremental closeness cost is formulated as the dimension-wise thresholded \(L^1\) distance between the current and target creations, given by,
\begin{equation}
    \label{eq:closeness-reward-sparse}
    d(\bfi, \bft) = \sum_{k \in n_s} \bm{1}_\varepsilon(\bfi_k - \bft_k),
\end{equation}
where \(\bfi, \bft \in \cS\) are the current and target creations respectively, \(\varepsilon\) is a small threshold, and \(\bm{1}_\varepsilon\) is the indicator function such that \(\bm{1}_\varepsilon(x) = 1\) if \(|x| > \varepsilon\) and \(0\) otherwise.

Using the closeness reward/cost formulations, we performed an extensive grid search over the many hyperparameters of the iCEM controller and the environment together to find the best combinations and study their effects.
To assess and compare the results in these experiments, we compared their cumulative rewards over the rollouts.

The controller was easily able to achieve the goal under dense closeness rewards for a wide range of hyperparameters and other than a few parameters such as the granularity of the grid, the step size, and the object persistency, we did not notice much difference in the performance of the controller over the other hyperparameters.
\figref{fig:closeness-rollouts} shows a sample closeness reward run for the dense closeness cost on the Tangram environment.

The tuning was more difficult under sparse incremental rewards, which required a higher planning horizon and more sampled trajectories to reach the goal.
This directly corresponds to a higher computational cost.

Generally, in a continuous Tangram grid, with a step size of \(4\), moving one object at a time for one action step, an iCEM controller with \(128\) trajectories and a minimum planning horizon of \(8\) with \(3\) inner iterations was able to reach the goal in \(30-60\) timesteps.

The cost aggregation functions \texttt{best} and \texttt{sum} were comparable in performance and performed better than \texttt{best-\emph{l}}, \texttt{last}, and \texttt{last-\emph{l}}.
We typically employed the \texttt{sum} aggregation function for our main simulations due to its summation operation which averages over the rewards in the planning horizon and makes it more robust to noise.
It essentially has a regularizing effect on the reward landscape.
In other runs, with other regularization techniques in place, we found the greedier \texttt{best} aggregation function to be more efficient in reaching the goal and escaping local minima.

The comparison results are omitted here for brevity. Please refer to the appendix sections \ref{sec:icem-details} and \ref{sec:environments-details} for more details about these hyperparameters of the iCEM controller and the environments respectively. It also lists the optimal and default values we observed and typically used in our simulations (unless specified otherwise).

From this analysis, we were able to gain an intuition of the minimal required set of hyperparameters that allowed the controller to solve the environment.
These minimal parameters were then used as a starting point for the simulations with the semantics entropy reward.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{images/closeness_trajectory_495.pdf}
    \caption{A successful closeness cost rollout in Tangram.}
    \label{fig:closeness-rollouts}
\end{figure}


% \subsubsection{Tuning Environment Parameters}
% \label{sec:env-hyperparameters}
% % render_kwargs.invert
% % render_kwargs.color

% \label{sec:sgw-hyperparameters}
% % width
% % x_step
% % render_delta
% % object_persistency
% % max_dist
% % control
% % control_boundaries


% \label{sec:tangram-hyperparameters}
% % flip
% % rotate
% % x_size
% % r_size
% % x_step
% % object_persistency
% % max_dist
% % control
% % control_boundaries
% % staging_boundaries


% \subsubsection{Tuning Controller Hyperparameters}
% \label{sec:icem-hyperparameters}
% % action_sampler_params.opt_iterations
% % action_sampler_params.init_std
% % action_sampler_params.elites_size
% % num_simulated_trajectories
% % horizon
% % cost_along_trajectory
% % discount_along_trajectory

% \todo{Talk about the different hyperparameters that were considered. Show histograms of only the significant parameters.}

% \newpage
\section{Improving CLIP Rewards}
\label{sec:improving-rewards}

We experimented with several methods to reduce the noise from CLIP.
The regularized semantics reward also has many hyperparameters that can be tuned to make the reward landscape smoother.
Namely, the temperature of the softmax function (\(T\)), the text baseline regularization strength (\(\alpha\)), the image baseline regularization strength (\(\beta\)), the use of negative embeddings, and tweaking the rendering function (adding texturing or modifying the images with other operations).

The complex nature of the reward landscape and the high dimensionality of the state space made it difficult to analyze the effects of these hyperparameters.
To gain an intuition over these parameters and reduce the search space, we used the best of the previous rollouts with the closeness reward from \secref{sec:closeness-rollouts} and ran post hoc inference on the resulting sequence of image observations using CLIP to calculate the trajectories of the resulting indirect semantics reward\footnote{The myriad of combinations due to the high dimensional space makes it infeasible to show all the subtleties of the interplay between these hyperparameters in a few figures.
Thus, we have additionally made the analysis available for the reader as an interactive notebook at \url{https://colab.research.google.com/drive/1UzKb5t5PDRO05GbSKpgzWd3DELfAzDxJ} or \url{https://t.ly/j84on}, where one can pick a combination of different hyperparameters, and prefixes and suffix to see their effects}.

Given a generic set of simple Tangram creations for the creative possibilities, we found that the choice of prefix and suffix could affect the reward trajectories significantly (see \figref{fig:prefix-suffix}).
Their performance is measured in terms of the mean-shifted cumulative rewards.

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{images/prefix-suffix.pdf}
    \caption{Effect of different prefixes and suffixes on semantics entropy reward trajectories in Tangram.}
    \label{fig:prefix-suffix}
\end{figure}
\vspace{-7pt}
The following sections on the specific hyperparameters discuss the results of these analyses.
To get the underlying trends in these parameters and find robust values that are agnostic to prompt engineering, i.e. the choice of prefix and suffix in formulating the text input, we averaged over our results across multiple choices and combinations of these text prefix-suffix pairs.

We observed clear trends in the effects of these hyperparameters on the reward landscape.
Consequently, we performed an ablation study on these hyperparameters independently in our analyses to gain insights into their effects.
Unless specified otherwise, we chose the best-performing values of hyperparameters not under consideration in that section.

In the figures shown in the following sections, we mostly plot the costs and cumulative semantics entropy costs as a measure of performance, which is the negative of the corresponding reward.
We use the terms \emph{cost} and \emph{reward} interchangeably.
Additionally, note that in the trajectory plots, the costs are filtered with a moving average filter of length \(3\) to reduce clutter and make the trends more visible.
The underlying unfiltered mean trajectories are underlaid in the same but fainter colors.

Ideally, lower cumulative costs indicate better performance, but it has to be interpreted with caution due to the subjective nature of the goal here (semantic expression). Even with a higher cumulative cost, the agent can reach a more semantically expressive state.
These effects are discussed accordingly in prose.

% The results in this section can be compared to the corresponding adversarial study results discussed in \secref{sec:adversarial-performance}.

% \newpage
\subsection{Effect of the Number of Creative Possibilities}
\label{sec:clip-categories}
To alleviate the problem of class preference in CLIP, we experimented with different numbers of categories/creative possibilities (\(c\)) for the semantics entropy reward.
\figref{fig:clip-categories} shows the effect of the number of categories on the entropy of CLIP inferences over a random image.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{images/category_comparison_tangram.pdf}
    \caption{Effect of the number of categories on CLIP inference distribution over a random image.}
    \label{fig:clip-categories}
\end{figure}
\vspace{-7pt}
We observed that too few categories can promote semantic bias and class preference in random images, but too many categories could potentially exacerbate the problem of noisy rewards since there are potentially more misleading distractions for the agent.

Additionally, it is important to choose categories that were semantically distinct and not too broad which was non-trivial.
Especially for the Tangram environment, in which the creations can be quite abstract, there is only a limited set of feasible categories.
This suggested that a moderate number of categories would be optimal.
For our simulations, we typically chose a set of \(10 \sim 20\) categories.

\subsection{Effect of Temperature}
\label{sec:reg-temperature}
% semantics_model_temperature

Temperature is a crucial hyperparameter for the softmax function in the semantics entropy reward as it directly controller the entropy of the CLIP distribution \eqref{eq:clip-dist}.
The original CLIP publication recommends a temperature of \(0.01\) for most use cases, which we also found to most often work best to reach a good optimum.
Temperatures up to \(0.02\) were good as well but any value below this range performed significantly worse as it adds to the noise of CLIP, and any temperature above this range rendered the reward landscape too smooth and the reward trajectories too flat leading to a loss of semantic inference from CLIP.
\figref{fig:clip-temperature} shows this effect, averaged over multiple choice of categories.
% This is a post hoc CLIP inference on trajectory samples from the closeness reward rollout experiments.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{images/temperature_comparison.pdf}
    \caption{Effect of temperature on semantics entropy reward trajectories.}
    \label{fig:clip-temperature}    
\end{figure}

\subsection{Effect of Baseline Regularization}
\label{sec:reg-alpha-beta}
% semantics_alpha_target

The regularization strengths of the text baseline (\(\alpha\)) and the image baseline (\(\beta\)) are arguably the most important hyperparameters for the regularized semantics entropy reward.
Baseline regularization helps by providing better directional guidance in the CLIP embedding space.

We compared the cumulative semantics entropy rewards of the post hoc inferences on closeness reward trajectories with different values of \(\alpha\) and \(\beta\) to find the optimal values and found the best values of the regularization strengths to be somewhere in the middle of the two extremes, with the optimal image baseline regularization strength a slightly higher than the text baseline regularization strength.
This is in line with the findings of \cite{vlmrm} in goal-conditioned reward settings.

\figref{fig:clip-alpha-beta} shows these results, averaged over multiple choice of prefixes and suffixes.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{images/alpha_beta_temp12avg_noneg.pdf}
    \caption{Effect of regularization strength on semantics entropy reward trajectories.}
    \label{fig:clip-alpha-beta}
\end{figure}


% \subsection{Effect of Changing Prefix and Suffix}
% \label{sec:prefix-suffix}
% % label_prefix
% % label_suffix
% Different prefixes/suffixes and combinations of them also affected the performance of CLIP.

% \begin{figure}[H]
%     \centering
%     \missingfigure{Effect of changing the prefix suffix.}
%     \caption{Effect of different prefixes and suffixes.}
%     \label{fig:prefix-suffix}
% \end{figure}

\subsection{Effect of Negative Embeddings}
\label{sec:negative-embeddings}
\cite{negprompt} used negative embeddings as target baselines in their goal-conditioned CLIP reward function to make the reward landscape smoother.
We experimented with this as well; instead of using the initial description of the environment as the text baseline, we used a negative formulation of this description.

We did not find this to be helpful in our experiments.
Instead, it seemed to flatten the reward landscape (see \figref{fig:baseline}).
We think this is a consequence of CLIP's language encoder being a bag-of-words model, which makes the negative embeddings essentially close to the target embeddings, and effectively zeros them out.

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{images/baseline_comparison.pdf}
    \caption{Effect of different text baselines on semantics entropy reward trajectories.}
    \label{fig:baseline}
\end{figure}

\subsection{Effect of Adding Post-Suffix}
\label{sec:post-suffix}
\cite{waffleclip} found that adding a \emph{post-}suffix to the label in the text input to CLIP improved the quality of the inferences.
This post-suffix consists of an additional concept for the class and a random string of characters.

Following these insights, we also experimented with adding random jibberish to the end of the label as a post-suffix, but we did not find it to affect the reward.
\figref{fig:post-suffix} shows the effect of different post-suffixes on the semantics entropy reward trajectories, averaged over combinations of prefixes and suffixes.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{images/post_suffix_comparison.pdf}
    \caption{Effect of different post-suffixes on semantics entropy reward trajectories.}
    \label{fig:post-suffix}
\end{figure}

% \begin{figure}[H]
%     \centering
%     \missingfigure{As a consolidation of everything before, a bar plot with ablations.}
%     \caption{Ablations of the different methods to improve semantics entropy reward.}
%     \label{fig:clip-ablation}
% \end{figure}

% \newpage
\subsection{Entropy Regularization}
\label{sec:entropy-regularization}
Another promising way to make the reward landscape of CLIP smoother is to fine-tune it with an additional entropy regularization loss over its output.
This is given by,
\begin{equation}
    \label{eq:entropy-regularization}
    L(\bmi, \bml) = L_{\iota}(\bmi, \bml) \underbrace{+ \kappa \sum_{\bfi_k \in \bmi}\sum_{\bfl_j \in \bml} p(\bfl_j; \bfi_k, \bml) \log p(\bfl_j; \bfi_k, \bml)}_{\text{Entropy Regularization}},\\
\end{equation}
where \(L_{\iota}\) is the contrastive cosine-similarity loss used to train CLIP, $\kappa$ is the regularization strength, and \(p(\bfi_k, \bml)\) is the classification probability distribution of image \(\bfi_k\) over the labels \(\bml\) predicted by CLIP.

We experimented with this regularization method to train a toy convolutional neural network (CNN) for classifying handwritten digits from the MNIST dataset \citep{mnist}, which we called \emph{flatnet}.
We trained this model with a modified MNIST training dataset augmented with varying sizes of random-image samples labeled with a uniform distribution over the categories.

This was quite effective in reducing the noise of the model; it seemed to relieve both of the problems from \secref{sec:clip-problems} -- the reward trajectory was smoother, and we observed less semantic bias in random images and even less class preference for random inputs.

Yet, we did not use it to fine-tune CLIP to constrain the scope of the project given the limited time.
More information on this analysis can be found in appendix \ref{sec:flatnet}.

% \newpage
\subsection{Adversarial Performance}
\label{sec:adversarial-performance}
To especially tackle the problem of semantic bias in random images, we collected samples of the false positive image observations from our rollouts, called \emph{adversarial observations} (\figref{fig:semantic-bias-random}), by filtering out the image observations with low entropy from all our runs and then manually removing the ones that were true positives.

\begin{figure}[h]
    \centering
    % \includegraphics[width=0.6\textwidth]{images/p_random_semantic_bias.png}
    \includegraphics[width=\textwidth]{images/adversarial_samples.pdf}
    \caption{Adversarial samples from simulations.}
    \label{fig:semantic-bias-random}
\end{figure}

Then, we searched for the semantics reward regularization hyperparameters that made CLIP less confident in the adversarial images while maintaining its inference for the truly semantically expressive images, i.e. increasing its specificity while maintaining its sensitivity.

We used the difference in mean entropy of distributions for the false positive and true positive images as a metric to gauge the performance of CLIP.
\figref{fig:clip-temperature-adversarial} to \ref{fig:post-suffix-adversarial} show the effect of temperature, regularization, negative embeddings, and post-suffixes on discerning true positives from false positives.

The results in this subsection can be compared to the corresponding results shown just before.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{images/temperature_adversarial.pdf}
    \caption{Effect of temperature on discerning true positives from false positives.}
    \label{fig:clip-temperature-adversarial}
\end{figure}

Higher temperatures of \(2\) were better at smoothing the reward landscape just enough to improve its adversarial performance.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{images/alpha_beta_adversarial.pdf}
    \caption{Effect of regularization strength on discerning true positives from false positives.}
    \label{fig:clip-alpha-beta-adversarial}
\end{figure}

Interestingly, the trends in the regularization strength of the text baseline (\(\alpha\)) and the image baseline (\(\beta\)) shown in \figref{fig:clip-alpha-beta-adversarial} are essentially opposite from the ones in \figref{fig:clip-alpha-beta} that showed the effect on the reward landscape.
These results are not at odds with each other, but rather complementary. 
As noted before, there is an anti-correlation between the reward sparseness and semantic bias in random images.

An optimal choice of baseline strength for reducing the sparsity of the goal-based reward landscape essentially improves the path to this goal by improving the gradients to reach this desired reward.
This means that it changes the reward at near-goal status in such a way that it is better at intimating the agent if it is close to its ultimate goal, i.e. it increases the similarity or correspondence of this state to the goal state.
This is in contrast to the adversarial performance where we desire to make this difference more pronounced.

Next, we looked at the effect of different text baselines and post-suffixes on the adversarial performance.
To visualize these results, we plot the mean entropies of the false positives and true positives for different combinations of prefixes and suffixes for the text input in a 2D space.
Ideally, we would like the points to live above the diagonal, i.e. the mean entropy of the false positives should be higher than the true positives.
Additionally, we plot boxplots of the difference in the mean entropies of the false positives and true positives.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{images/baseline_adversarial_2.pdf}
    \caption{Effect of different text baselines on discerning true positives from false positives.}
    \label{fig:baseline-adversarial}
\end{figure}

\figref{fig:baseline-adversarial} shows the effect of different text baselines on the adversarial performance.
Negative embeddings as we have seen before, flatten the reward landscape uniformly and as a consequence increase the entropy of the true positives as well.
This results in no performance improvement in the adversarial case.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{images/post-suffix_adversarial_2.pdf}
    \caption{Effect of different post-suffixes on discerning true positives from false positives.}
    \label{fig:post-suffix-adversarial}
\end{figure}

\figref{fig:post-suffix-adversarial} shows the effect of different post-suffixes on the adversarial performance.
Since we were not sure of the exact mechanism for choosing a post-suffix that would work well in general, for this analysis, to ensure more exhaustive and rigorous tests, we also used different formulations of the post-suffix and considered cases such as having the same random post-suffix across all the categories for every prefix-suffix combination or having a random one for each of the categories and combinations.
As before, we see no net effect in adding or omitting the post-suffix, but just a difference in variance.

% \begin{figure}[H]
%     \centering
%     \includegraphics[width=\textwidth]{images/p_subplots/rair_.pdf}
%     \includegraphics[width=\textwidth]{images/p_subplots/entropy_.pdf}
%     \includegraphics[trim=1.6cm 0cm 0cm 0cm, width=\textwidth]{images/p_subplots/distribution_.pdf}
%     \includegraphics[width=\textwidth]{images/p_subplots/snapshots_.pdf}
%     \caption{A sample rollout distribution plot.}
% \end{figure}

% \newpage
\subsection{Effect of Image Texturing}
\label{sec:image-texturing}
We also experimented with adding textures in the environment renderings
in hopes of bringing these image inputs closer to in-distribution for CLIP to improve its inference quality.
This was done by adding a constant shading to the Tangram polygons (see \figref{fig:texturing}).
We did not find a significant improvement in the reward landscape with this texturing.
\figref{fig:texturing-operations} shows its effects.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{images/hatched.pdf}
    \caption{Texturing in Tangram.}
    \label{fig:texturing}
\end{figure}

\subsection{Effect of Image Operations}
\label{sec:image-operations}
% Shearing and Hatching
We also had the idea to use subtle image operations like shearing before inference.
We expected the true positive semantic inferences to be invariant to these operations, but the false positives to be reduced, thus reducing the noise in the reward landscape.
% This is compared together with the texturing operations in \figref{fig:texturing-operations} and \figref{fig:texturing-operations-adversarial} to study their effects on the reward landscape and the adversarial performance respectively.

Shearing the images before inference did improve the quality of the inferences slightly by smoothing the reward landscape as shown in \figref{fig:texturing-operations}.
It had a significant performance improvement in the adversarial case as well (\figref{fig:texturing-operations-adversarial}), but it also affected the confidence in the true positive images, thus leading to lower performance by our cumulative reward/cost metric (\figref{fig:texturing-operations}).

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{images/texturing_operations_comparison.pdf}
    \caption{Effect of texturing and image operations on semantics entropy reward trajectories.}
    \label{fig:texturing-operations}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{images/texturing-operations_adversarial_2.pdf}
    \caption{Effect of image operations on discerning true positives from false positives.}
    \label{fig:texturing-operations-adversarial}
\end{figure}

To show the interdependence of the hyperparameters discussed above, \figref{fig:baseline_sheared_adversarial} and \figref{fig:post-suffix_sheared_adversarial}  compare the adversarial performance of the sheared images across negative embeddings and different post-suffix patterns respectively.

We observe that the shearing operation when used with baselines based on initial states is very helpful in improving the adversarial performance of CLIP as seen by the points gathering in the upper right corner of \figref{fig:baseline_sheared_adversarial}.

Yet, in the following sections, we did not use this method, because of the associated added computational costs of rendering, and because we could get a similar effect by tuning the baseline regularization strengths and the temperature.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{images/baseline_sheared_adversarial_2.pdf}
    \caption{Effect of different text baselines on discerning true positives from false positives with sheared images.}
    \label{fig:baseline_sheared_adversarial}
\end{figure}

The addition of post-suffixes in the sheared image case impairs the adversarial performance, as shown in \figref{fig:post-suffix_sheared_adversarial}.
This result surfaces their originally intended regularization/smoothing effect which is not as visibly pronounced in the non-sheared case in the study of its effect on the reward trajectories (\figref{fig:post-suffix}).

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{images/post-suffix_sheared_adversarial_12.pdf}
    \caption[Effect of different post-suffixes on discerning true positives from false positives with sheared images.]{Effect of different post-suffixes on discerning true positives from false positives with the sheared image. Note that the temperature here is lower, \(T = 0.012\), the points gather in the upper right corner at higher temperatures.}
    \label{fig:post-suffix_sheared_adversarial}
\end{figure}

% \newpage
\section{Simulations using the Semantics Reward}
\label{sec:simulations}
% Manual ranking and entropy ranking

Using the suitable combinations of the controller and environment hyperparameters from \secref{sec:closeness-rollouts} and insights into the regularized semantics entropy reward hyperparameters from \secref{sec:improving-rewards}, we ran simulations (rollouts) with the semantics reward to generate creations in the environments.
We were generally able to consistently generate meaningful creations in both environments, but the quality of the creations varied with the exact choice of these reward hyperparameters.

These simulations were computationally expensive and required significant time to run.
Thus, we were limited in our ability to further investigate, confirm, and fine-tune all the hyperparameters exhaustively enough. 
Consequently, we focused our analyses on the main few -- the baseline regularization strengths and the effect of the additional regularity reward, RaIR.
We showcase the results of these analyses in the following sections.
All figures show the mean cost trajectories over the rollouts for \(10\) different random seeds for each hyperparameter combination.

While we also ran enough simulations to confirm our intuitions about the other hyperparameters, only a few of our analyses used enough restarts with different random seeds to have the statistical power to make justified general claims.
These additional results are available in \secref{sec:sgw-semantics-additional} of the appendix.

To evaluate the obtained final creations, in addition to the cumulative reward, we also asked three human evaluators to manually rank all the final creations on a scale of \(1\) to \(5\) based on their apparent underlying semantic expressiveness and subjective feel/quality of the patterns.
In the following parts, shown creations (in Tangram) not particularly interesting according to these rankings are desaturated to avoid visual clutter.
Interesting creations based on these results are curated in the gallery in \secref{sec:gallery}\footnote[1]{Animations and summary graphs of all the simulations and creations referenced in this thesis are available on \url{https://drive.google.com/drive/folders/1RdG86GLLujH3z6eDRIvdaczokpr8krnc} or \url{https://t.ly/49O3h}.}.


% B HUMAN EVALUATION
% In these cases, goal baseline regularization does not improve performance.
% Together with the results in Figure 4a, this could suggest that goal-baseline regularization is more useful for smaller CLIP models than for larger CLIP models.
% Alternatively, it is possible that the improvements to the reward model obtained by goal-baseline regularization are too small to lead to noticeable performance increases in the trained agents for the failing humanoid tasks.
% Unfortunately, a more thorough study of this was infeasible due to the cost associated with human evaluations.

% \subsection{Evaluating Regularized Entropy Reward Formulation in Goal-Conditioned Tasks}
% \label{sec:evaluating-regularized-entropy}
% % alpha_target
% % beta_image

\subsection{Effect of Regularization Strengths}
\label{sec:alpha-beta-semantics}
% Model Search
We ran simulations with different regularization strengths to reconcile their effects with the results from the previous sections.
\figref{fig:alpha-beta-semantics} shows the heatmap of the mean cumulative rewards for different combinations of the regularization strengths.
% For standard deviations, please see \figref{fig:alpha_beta-semantics_std_rair}.

We find that non-zero regularization strengths (\(\alpha\) and/or \(\beta\)) do improve the performance, with mid-range values \((\alpha, \beta) = (0.3, 0
3)\) having the best performance by these metrics.
This corroborates the results in the post hoc trajectory analysis study in \secref{sec:reg-alpha-beta}.

% alpha_beta-semantics_rair.pdf -- heatmap
\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{images/alpha_beta-semantics_rair.pdf}
    \caption{Performance of regularization strengths on semantics reward.}
    \label{fig:alpha-beta-semantics}
\end{figure}

\figref{fig:alpha-beta-trajectories} shows the effect of regularization strengths on the semantics entropy cost trajectories and \figref{fig:alpha-beta-samples} shows some samples of the creations with different regularization strengths.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{images/alpha_beta_comparison_rair.pdf}
    \caption{Effect of regularization strengths on semantics entropy reward trajectories.}
    \label{fig:alpha-beta-trajectories}
    \vspace{12pt}
    \includegraphics[width=0.8\textwidth]{images/alpha_beta_rair_samples.pdf}
    \caption{Samples of creations with different regularization strengths.}
    \label{fig:alpha-beta-samples}
\end{figure}

Noticeably, the three cost trajectories with high image baseline regularization strength \(\beta = 0.5\) are too flat and seem to converge at entropy values greater than \(0\).
For \((\alpha, \beta) = (0.5, 0.3)\), the cost trajectory seems more sparse and noisy,
For other combinations of \((\alpha, \beta)\) -- \(()\), we do not see a significant difference in the performance or quality of creations.

\newpage
\subsection{Effect of RaIR}
\label{sec:effect-rair}
% compression_precision
% 1 / semantics_reward_scale

Throughout our simulations, we noticed a high degree of regularity in semantic creations and a high correlation between the RaIR and the semantics entropy reward. 
Indeed, we found that creations simulated with RaIR were much more recognizable than without it.
\figref{fig:rair} shows the effect and \figref{fig:rair-samples} shows some samples of the creations with and without RaIR.

% rair_comparison.pdf
\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{images/rair_comparison.pdf}
    \caption{Effect of RaIR on semantics entropy reward in Tangram.}
    \label{fig:rair}
    \vspace{12pt}
    \includegraphics[width=\textwidth]{images/rair_samples.pdf}
    \caption{Samples of creations with and without RaIR.}
    \label{fig:rair-samples}
\end{figure}

RaIR was particularly helpful in the beginning of the planning as the agent starts building regular structures to bring it closer to a semantically meaningful state which is then further refined by the semantics entropy reward.

Adding the structural bias from RaIR helped with grounding and coalescing the creations in more human-recognizable forms and patterns.

\todo{talk about observations of optimizing regularity after semantics}
\todo{talk about the weight of RaIR}
\todo{Creations are neater}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{images/sim_rair_later_bird.pdf}
    \caption{Simulation on Tangram with the complete semantics reward leading to a bird creation.}
    \label{fig:sim}
\end{figure}

We also tested this in ShapeGridWorld, where the effect of RaIR was only marginal and only affected when the pixels had grayscale values.

\figref{fig:rair-color-sgw} and \figref{fig:rair-samples-color-sgw} show the effect of RaIR on ShapeGridWorld with grayscale pixels.

% rair_comparison_sgw_color.pdf
\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{images/rair_comparison_sgw_color_cropped.pdf}
    \caption{Effect of RaIR on ShapeGridWorld with grayscale pixels.}
    \label{fig:rair-color-sgw}
    \vspace{12pt}
    \includegraphics[width=0.5\textwidth]{images/rair_samples_sgw_color.pdf}
    \caption{Samples of creations with and without RaIR in ShapeGridWorld with grayscale pixels.}
    \label{fig:rair-samples-color-sgw}
    % rair_comparison_sgw_categories.pdf
    \vspace{12pt}
    \includegraphics[width=\textwidth]{images/rair_comparison_sgw_categories_cropped.pdf}
    \caption{Effect of RaIR on ShapeGridWorld.}
    \label{fig:rair-sgw}
    \vspace{12pt}
    \includegraphics[width=\textwidth]{images/rair_samples_sgw_categories.pdf}
    \caption[Samples of creations with and without RaIR in ShapeGridWorld.]{Samples of creations with and without RaIR in ShapeGridWorld. % Using the \texttt{best} aggregation function.
    }
    \label{fig:rair-samples-sgw}
\end{figure}

% \subsection{Final Simulations on ShapeGridWorld}
% \label{sec:sgw-semantics}

% While we were consistently able to get good creations on Tangram, the results on ShapeGridWorld due to the many degrees of freedom were more varied.

% We ran final simulations on ShapeGridWorld with the best hyperparameters from the previous analyses.
% \figref{fig:sgw-trajectories} shows the effect of regularization strengths on the semantics entropy reward trajectories.

Results for additional simulations on ShapeGridWorld are given in \secref{sec:sgw-semantics-additional}.
